{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ea8d5b39270440f5a1a98310c48582c2",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6dd8b2343c004e968249eadc0be99fd9",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a2c8a570873045f7b12ba966ca1dc26e",
    "deepnote_app_block_visible": true,
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T13:53:12.522453700Z",
     "start_time": "2024-04-12T13:53:04.301993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dotenv file\n"
     ]
    }
   ],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Visualization Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import custom functions\n",
    "import env_functions as ef\n",
    "import s3_functions as sf\n",
    "import common_functions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modeling Libraries\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Import Evaluation Libraries\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the environment and get appropriate vars\n",
    "deepnote, env_vars = ef.load_env_vars()\n",
    "\n",
    "# Iterate through the vars and set them as global vars\n",
    "for var_name, var in env_vars.items():\n",
    "    globals()[var_name] = var\n",
    "\n",
    "# If not in the DeepNote environment, create a dict for aws creds\n",
    "#   that were located in the environment file.  This will be passed\n",
    "#   to all aws s3 functions.\n",
    "if not deepnote:\n",
    "    aws_env_vars = {\n",
    "        'access_key_id': aws_access_key_id,\n",
    "        'secret_access_key': aws_secret_access_key,\n",
    "        'bucket_name': s3_bucket_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas Configs\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=FutureWarning)\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# MapBox Token for Plotly Maps\n",
    "px.set_mapbox_access_token(os.environ.get(\"MAPBOX_TOKEN\"))\n",
    "\n",
    "# Scikit Learn Configs\n",
    "set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_model(cols=[], Xtt=None, ytt=None, Xht=None, yht=None, Xvt=None, yvt=None, train=True, val=True, holdout=True, feature=False, model_type='xgb'):\n",
    "    \"\"\"\n",
    "    Train a model with the default hyperparameters.  If the \"feature\" parameter is true, a combination of \n",
    "    all features for Xtt are tested and the results returned in a dataframe.  If the \"cols\" parameter has\n",
    "    a list of features, that will be used for the feature combinations, instead of the full list of features\n",
    "    from the Xtt (X_train_trans) dataframe.\n",
    "\n",
    "    params:\n",
    "    cols: List of columns to use\n",
    "    Xtt: Training dataframe\n",
    "    ytt: Training target\n",
    "    Xht: Holdout dataframe\n",
    "    yht: Holdout target\n",
    "    Xvt: Validation dataframe\n",
    "    yvt: Validation target\n",
    "    feature: If True, test model on all feature combinations\n",
    "    model_type: 'xgb', 'lgbm', 'rf' or 'hgb'\n",
    "\n",
    "    returns: Pandas Dataframe with model results for feature combinations\n",
    "    \"\"\"\n",
    "    \n",
    "    # If columns are passed in, use them.  Otherwise, use all columns\n",
    "    if len(cols) > 0:\n",
    "        Xtt = Xtt[cols]\n",
    "        Xht = Xht[cols]\n",
    "        Xvt = Xvt[cols]        \n",
    "    \n",
    "    # Create the model\n",
    "    if model_type == 'xgb':\n",
    "        model = xgb.XGBRegressor(objtive='reg:squarederror', eval_metric='mae')\n",
    "    elif model_type == 'lgbm':\n",
    "        model = lgb.LGBMRegressor(verbose=-1, random_state=42, n_jobs=-1)\n",
    "    elif model_type == 'rf':\n",
    "        model = RandomForestRegressor()\n",
    "    elif model_type == 'hgb':\n",
    "        model = HistGradientBoostingRegressor()\n",
    "    else:\n",
    "        raise ValueError(\"model_type: must be a string that is either 'xgb', 'lgbm', 'rf', or 'hgb\")\n",
    "    \n",
    "    # Fit the model and get the cross_val_score \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    mae_scores = -cross_val_score(model, Xtt, ytt, cv=kf, scoring='neg_mean_absolute_error')\n",
    "    mean_mae = mae_scores.mean()\n",
    "    std_mae = mae_scores.std()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Model 5 Fold CV Mean MAE: {mean_mae:.4f}\")\n",
    "    print(f\"Model 5 Fold CV Std MAE: {std_mae:.4f}\")\n",
    "    print(f\"Model 5 Fold CV MAE Scores: {mae_scores}\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model.fit(Xtt, ytt)\n",
    "    \n",
    "    # Evaluate the model on the training, validation and holdout data\n",
    "    if train:\n",
    "        # Evaluation on train data\n",
    "        train_pred = model.predict(Xtt)\n",
    "        train_pred = np.clip(train_pred, 0, 100)\n",
    "        train_mae = mean_absolute_error(ytt, train_pred)\n",
    "        train_mse = mean_squared_error(ytt, train_pred)\n",
    "        train_rmse = mean_squared_error(ytt, train_pred, squared=False)\n",
    "        train_rsq = r2_score(ytt, train_pred)\n",
    "        print(\" \")\n",
    "        print(f\"Train Mean Absolute Error: {train_mae:.4f}\")\n",
    "        print(f\"Train Mean Squared Error: {train_mse:.4f}\")\n",
    "        print(f\"Train Root Mean Squared Error: {train_rmse:.4f}\")\n",
    "        print(f\"Train R^2 Score: {train_rsq:.4f}\")\n",
    "\n",
    "    if val:\n",
    "        # Evaluation on validation data\n",
    "        val_pred = model.predict(Xvt)\n",
    "        val_pred = np.clip(val_pred, 0, 100)\n",
    "        val_mae = mean_absolute_error(yvt, val_pred)\n",
    "        val_mse = mean_squared_error(yvt, val_pred)\n",
    "        val_rmse = mean_squared_error(yvt, val_pred, squared=False)\n",
    "        val_rsq = r2_score(yvt, val_pred)\n",
    "        print(\" \")\n",
    "        print(f\"Validation Mean Absolute Error: {val_mae:.4f}\")\n",
    "        print(f\"Validation Mean Squared Error: {val_mse:.4f}\")\n",
    "        print(f\"Validation Root Mean Squared Error: {val_rmse:.4f}\")\n",
    "        print(f\"Validation R^2 Score: {val_rsq:.4f}\")\n",
    "\n",
    "    if holdout:\n",
    "        # Evaluation on holdout data\n",
    "        holdout_pred = model.predict(Xht)\n",
    "        holdout_pred = np.clip(holdout_pred, 0, 100)\n",
    "        holdout_mae = mean_absolute_error(yht, holdout_pred)\n",
    "        holdout_mse = mean_squared_error(yht, holdout_pred)\n",
    "        holdout_rmse = mean_squared_error(yht, holdout_pred, squared=False)\n",
    "        holdout_rsq = r2_score(yht, holdout_pred)\n",
    "        print(\" \")\n",
    "        print(f\"Holdout Mean Absolute Error: {holdout_mae:.4f}\")\n",
    "        print(f\"Holdout Mean Squared Error: {holdout_mse:.4f}\")\n",
    "        print(f\"Holdout Root Mean Squared Error: {holdout_rmse:.4f}\")\n",
    "        print(f\"Holdout R^2 Score: {holdout_rsq:.4f}\")\n",
    "\n",
    "    if feature:\n",
    "        # Evaluation on train data\n",
    "        train_pred = model.predict(Xtt)\n",
    "        train_pred = np.clip(train_pred, 0, 100)\n",
    "        baseline_mae_train = mean_absolute_error(ytt, train_pred)\n",
    "    \n",
    "        # Evaluation on validation data\n",
    "        val_pred = model.predict(Xvt)\n",
    "        val_pred = np.clip(val_pred, 0, 100)\n",
    "        baseline_mae_val = mean_absolute_error(yvt, val_pred)\n",
    "        \n",
    "        # Create a list of all features\n",
    "        feat_list = [x for x in Xtt.columns]\n",
    "    \n",
    "        # Now create combos of features\n",
    "        # Range starts at 10 to skip features\n",
    "        # Based on testing of CARB data, lowest number of features is 10 in the top100 models\n",
    "        # Based on testing of SEAA data, it prefers lower numberes of features ...\n",
    "        #feat_combo_list = [combo for r in range(10, len(feat_list) + 1)\n",
    "        feat_combo_list = [combo for r in range(2, 10)\n",
    "                          for combo in itertools.combinations(feat_list, r)]\n",
    "    \n",
    "        # Itertools combinations() creates tuples.\n",
    "        #   Convert each combination from a tuple to a list for feature building\n",
    "        feat_combo_list = [list(combo) for combo in feat_combo_list]\n",
    "\n",
    "        print(\"Number of Feature Combinations: \", len(feat_combo_list))\n",
    "    \n",
    "        \n",
    "        # Create the feature loop\n",
    "        feature_results_list = []\n",
    "    \n",
    "        # Feat loop\n",
    "        for feature in feat_combo_list:\n",
    "            # use feature cols\n",
    "            modified_X_train_trans = Xtt[feature]\n",
    "            modified_X_val_trans = Xvt[feature]\n",
    "    \n",
    "            # Fit the model with features\n",
    "            model.fit(modified_X_train_trans, ytt)\n",
    "    \n",
    "            # Evaluation on train data\n",
    "            modified_train_predictions = model.predict(modified_X_train_trans)\n",
    "            modified_train_predictions = np.clip(modified_train_predictions, 0, 100)\n",
    "            modified_mae_train = mean_absolute_error(ytt, modified_train_predictions)\n",
    "    \n",
    "            # Evaluation on validation data\n",
    "            modified_val_predictions = model.predict(modified_X_val_trans)\n",
    "            modified_val_predictions = np.clip(modified_val_predictions, 0, 100)\n",
    "            modified_mae_val = mean_absolute_error(yvt, modified_val_predictions)\n",
    "    \n",
    "            # Calculate MAE changes\n",
    "            mae_change_train = baseline_mae_train - modified_mae_train\n",
    "            mae_change_val = baseline_mae_val - modified_mae_val\n",
    "    \n",
    "            feature_result_dict = {\n",
    "                'Features': \", \".join(feature),\n",
    "                'Train_MAE': modified_mae_train,\n",
    "                'Train_MAE_Change': mae_change_train,\n",
    "                'Train_MAE_Pct_Change': 100 * (1 - (modified_mae_train / baseline_mae_train)),\n",
    "                'Val_MAE': modified_mae_val,\n",
    "                'Val_MAE_Change': mae_change_val,\n",
    "                'Val_MAE_Pct_Change': 100 * (1 - (modified_mae_val / baseline_mae_val))\n",
    "            }\n",
    "    \n",
    "            # Append the results to the list\n",
    "            feature_results_list.append(feature_result_dict)\n",
    "    \n",
    "        # Create a dataframe from the results list\n",
    "        feature_df = pd.DataFrame(feature_results_list)\n",
    "    \n",
    "        return feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_s3_path = 'data/Feature_Selection'\n",
    "features_dns3_path = \"/datasets/s3/data/Feature_Selection\"\n",
    "features_dn_path = \"/work/data/Feature_Selection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARB_Xtt, CARB_Xvt, CARB_Xht, CARB_ytt, CARB_yvt, CARB_yht = cf.import_data(location_name='CARB')\n",
    "SEAA_Xtt, SEAA_Xvt, SEAA_Xht, SEAA_ytt, SEAA_yvt, SEAA_yht = cf.import_data(location_name='SEAA')\n",
    "GLOB_Xtt, GLOB_Xvt, GLOB_Xht, GLOB_ytt, GLOB_yvt, GLOB_yht = cf.import_data(location_name='GLOB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Base Model Comparisons - XGBoost\")\n",
    "print(\"GLOB\")\n",
    "feature_model(Xtt=GLOB_Xtt, Xvt=GLOB_Xvt, Xht=GLOB_Xht, ytt=GLOB_ytt, yvt=GLOB_yvt, yht=GLOB_yht, feature=False, model_type='xgb')\n",
    "print(\"-\"*20)\n",
    "print(\"SEAA\")\n",
    "feature_model(Xtt=SEAA_Xtt, Xvt=SEAA_Xvt, Xht=SEAA_Xht, ytt=SEAA_ytt, yvt=SEAA_yvt, yht=SEAA_yht, feature=False, model_type='xgb')\n",
    "print(\"-\"*20)\n",
    "print(\"CARB\")\n",
    "feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='xgb')\n",
    "print(\"-\"*20)\n",
    "print(\"Time taken:\", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Base Model Comparisons - LightGBM\")\n",
    "print(\"GLOB\")\n",
    "feature_model(Xtt=GLOB_Xtt, Xvt=GLOB_Xvt, Xht=GLOB_Xht, ytt=GLOB_ytt, yvt=GLOB_yvt, yht=GLOB_yht, feature=False, model_type='lgbm')\n",
    "print(\"-\"*20)\n",
    "print(\"SEAA\")\n",
    "feature_model(Xtt=SEAA_Xtt, Xvt=SEAA_Xvt, Xht=SEAA_Xht, ytt=SEAA_ytt, yvt=SEAA_yvt, yht=SEAA_yht, feature=False, model_type='lgbm')\n",
    "print(\"-\"*20)\n",
    "print(\"CARB\")\n",
    "feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='lgbm')\n",
    "print(\"-\"*20)\n",
    "print(\"Time taken:\", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Base Model Comparisons - RandomForest\")\n",
    "print(\"GLOB\")\n",
    "feature_model(Xtt=GLOB_Xtt, Xvt=GLOB_Xvt, Xht=GLOB_Xht, ytt=GLOB_ytt, yvt=GLOB_yvt, yht=GLOB_yht, feature=False, model_type='rf')\n",
    "print(\"-\"*20)\n",
    "print(\"SEAA\")\n",
    "feature_model(Xtt=SEAA_Xtt, Xvt=SEAA_Xvt, Xht=SEAA_Xht, ytt=SEAA_ytt, yvt=SEAA_yvt, yht=SEAA_yht, feature=False, model_type='rf')\n",
    "print(\"-\"*20)\n",
    "print(\"CARB\")\n",
    "feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='rf')\n",
    "print(\"-\"*20)\n",
    "print(\"Time taken:\", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HistGradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "print(\"Base Model Comparisons - HistGradientBoosting\")\n",
    "print(\"GLOB\")\n",
    "feature_model(Xtt=GLOB_Xtt, Xvt=GLOB_Xvt, Xht=GLOB_Xht, ytt=GLOB_ytt, yvt=GLOB_yvt, yht=GLOB_yht, feature=False, model_type='hgb')\n",
    "print(\"-\"*20)\n",
    "print(\"SEAA\")\n",
    "feature_model(Xtt=SEAA_Xtt, Xvt=SEAA_Xvt, Xht=SEAA_Xht, ytt=SEAA_ytt, yvt=SEAA_yvt, yht=SEAA_yht, feature=False, model_type='hgb')\n",
    "print(\"-\"*20)\n",
    "print(\"CARB\")\n",
    "feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='hgb')\n",
    "print(\"-\"*20)\n",
    "print(\"Time taken:\", datetime.datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of Base Model Training on Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of the base model testing for regions indicates that out of the 4 decision tree based regressors, we should not concentrate on building custom feature sets for RandomForestRegressor due to a lack of reasonable times to execute.  At a time of 3 minutes and 45 seconds to build and score 3 models (compared to 1.6 seconds for LightGBM), the amount of time required to train against nearly 600k models would be prohibitive. Additionally, we should also not focus on HistGradientBoostingRegressor for different reasons.  While tuning would certainly improve its MAE scores, it did not perform better than XGBoost or LightGBM, and it took 324% more time than LightGBM and 235% longer than XGBoost.  Additionally, HistGradientBoostingRegressor does not have a feature importance function, making it more difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of continuing our analysis, we'll focus on either XGBoost or LightGBM, given these results.  Custom features for those models will be built out below for the Caribbean, South East Asia & Australia and Global regions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caribbean Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Feature Selection DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes approximately 30 hours to execute on a 32 core x 5GHz machine\n",
    "# CARB_XGB_feature_df = feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='xgb')\n",
    "# CARB_LGBM_feature_df = feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='lgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if writing out data\n",
    "# if deepnote:\n",
    "#     # Deep Note Local\n",
    "#     CARB_XGB_feature_df.to_parquet(f\"{features_dn_path}/XGBoost_feature_selection_CARB.parquet\")\n",
    "#     CARB_LGBM_feature_df.to_parquet(f\"{features_dn_path}/LightGBM_feature_selection_CARB.parquet\")\n",
    "\n",
    "#     # S3 Integration\n",
    "#     CARB_XGB_feature_df.to_parquet(f\"{features_dns3_path}/XGBoost_feature_selection_CARB.parquet\")\n",
    "#     CARB_LGBM_feature_df.to_parquet(f\"{features_dns3_path}/LightGBM_feature_selection_CARB.parquet\")\n",
    "\n",
    "# else:\n",
    "#     sf.write_to_s3(file_path=f\"{features_s3_path}/XGBoost_feature_selection_CARB.parquet\", data=CARB_XGB_feature_df, **aws_env_vars)\n",
    "#     sf.write_to_s3(file_path=f\"{features_s3_path}/LightGBM_feature_selection_CARB.parquet\", data=CARB_LGBM_feature_df, **aws_env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DataFrames from disk if available instead of building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    CARB_XGB_feature_df = pd.read_parquet(f\"{features_dn_path}/XGBoost_feature_selection_CARB.parquet\")\n",
    "    CARB_LGBM_feature_df = pd.read_parquet(f\"{features_dn_path}/LightGBM_feature_selection_CARB.parquet\")\n",
    "else:\n",
    "    CARB_XGB_feature_df = pd.read_parquet(sf.load_from_s3(file_path=f\"{features_s3_path}/XGBoost_feature_selection_CARB.parquet\", **aws_env_vars))\n",
    "    CARB_LGBM_feature_df = pd.read_parquet(sf.load_from_s3(file_path=f\"{features_s3_path}/LightGBM_feature_selection_CARB.parquet\", **aws_env_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 features that were tested where the \"Val_MAE_Change\" is greater than or equal to 0\n",
    "# This will show the best combination of features that produce the lowest MAE score\n",
    "CARB_XGB_feature_df[CARB_XGB_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show the top 10 features that were tested where the \"Val_MAE_Change\" is greater than or equal to 0\n",
    "# Count the number of features that were used to generate that MAE score\n",
    "CARB_XGB_feature_df.sort_values(by=['Val_MAE'], ascending=True).head(10)['Features'].str.count(\",\") + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARB_XGB_feature_list = CARB_XGB_feature_df[CARB_XGB_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(1)['Features'].str.split(',').to_list()\n",
    "CARB_XGB_feature_list = CARB_XGB_feature_list[0]\n",
    "\n",
    "CARB_LGBM_feature_list = CARB_LGBM_feature_df[CARB_LGBM_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(1)['Features'].str.split(',').to_list()\n",
    "CARB_LGBM_feature_list = CARB_LGBM_feature_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    with open(f'{features_dn_path}CARB_XGB_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(CARB_XGB_feature_list, f)\n",
    "    with open(f'{features_dn_path}CARB_LGBM_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(CARB_LGBM_feature_list, f)\n",
    "    with open(f'{features_dns3_path}CARB_XGB_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(CARB_XGB_feature_list, f)\n",
    "    with open(f'{features_dns3_path}CARB_LGBM_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(CARB_LGBM_feature_list, f)\n",
    "else:\n",
    "    sf.write_to_s3(file_path=f\"{features_s3_path}/CARB_XGB_feat_list.pkl\", data=CARB_XGB_feature_list, pickle_file=True, **aws_env_vars)\n",
    "    sf.write_to_s3(file_path=f\"{features_s3_path}/CARB_LGBM_feat_list.pkl\", data=CARB_LGBM_feature_list, pickle_file=True, **aws_env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# South East Asia and Australia Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Feature Selection DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes approximately 30 hours to execute on a 32 core x 5GHz machine\n",
    "# SEAA_XGB_feature_df = feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='xgb')\n",
    "# SEAA_LGBM_feature_df = feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='lgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if writing out data\n",
    "# if deepnote:\n",
    "#     # Deep Note Local\n",
    "#     SEAA_XGB_feature_df.to_parquet(f\"{features_dn_path}/XGBoost_feature_selection_SEAA.parquet\")\n",
    "#     SEAA_LGBM_feature_df.to_parquet(f\"{features_dn_path}/LightGBM_feature_selection_SEAA.parquet\")\n",
    "\n",
    "#     # S3 Integration\n",
    "#     SEAA_XGB_feature_df.to_parquet(f\"{features_dns3_path}/XGBoost_feature_selection_SEAA.parquet\")\n",
    "#     SEAA_LGBM_feature_df.to_parquet(f\"{features_dns3_path}/LightGBM_feature_selection_SEAA.parquet\")\n",
    "\n",
    "# else:\n",
    "#     sf.write_to_s3(file_path=f\"{features_s3_path}/XGBoost_feature_selection_SEAA.parquet\", data=SEAA_XGB_feature_df, **aws_env_vars)\n",
    "#     sf.write_to_s3(file_path=f\"{features_s3_path}/LightGBM_feature_selection_SEAA.parquet\", data=SEAA_LGBM_feature_df, **aws_env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DataFrames from disk if available instead of building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    SEAA_XGB_feature_df = pd.read_parquet(f\"{features_dn_path}/XGBoost_feature_selection_SEAA.parquet\")\n",
    "    SEAA_LGBM_feature_df = pd.read_parquet(f\"{features_dn_path}/LightGBM_feature_selection_SEAA.parquet\")\n",
    "else:\n",
    "    SEAA_XGB_feature_df = pd.read_parquet(sf.load_from_s3(file_path=f\"{features_s3_path}/XGBoost_feature_selection_SEAA.parquet\", **aws_env_vars))\n",
    "    SEAA_LGBM_feature_df = pd.read_parquet(sf.load_from_s3(file_path=f\"{features_s3_path}/LightGBM_feature_selection_SEAA.parquet\", **aws_env_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 features that were tested where the \"Val_MAE_Change\" is greater than or equal to 0\n",
    "# This will show the best combination of features that produce the lowest MAE score\n",
    "SEAA_XGB_feature_df[SEAA_XGB_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 features that were tested where the \"Val_MAE_Change\" is greater than or equal to 0\n",
    "# Count the number of features that were used to generate that MAE score\n",
    "SEAA_XGB_feature_df.sort_values(by=['Val_MAE'], ascending=True).head(10)['Features'].str.count(\",\") + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEAA_XGB_feature_list = SEAA_XGB_feature_df[SEAA_XGB_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(1)['Features'].str.split(',').to_list()\n",
    "SEAA_XGB_feature_list = SEAA_XGB_feature_list[0]\n",
    "\n",
    "SEAA_LGBM_feature_list = SEAA_LGBM_feature_df[SEAA_LGBM_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(1)['Features'].str.split(',').to_list()\n",
    "SEAA_LGBM_feature_list = SEAA_LGBM_feature_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    with open(f'{features_dn_path}SEAA_XGB_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(SEAA_XGB_feature_list, f)\n",
    "    with open(f'{features_dn_path}SEAA_LGBM_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(SEAA_LGBM_feature_list, f)\n",
    "    with open(f'{features_dns3_path}SEAA_XGB_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(SEAA_XGB_feature_list, f)\n",
    "    with open(f'{features_dns3_path}SEAA_LGBM_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(SEAA_LGBM_feature_list, f)\n",
    "else:\n",
    "    sf.write_to_s3(file_path=f\"{features_s3_path}/SEAA_XGB_feat_list.pkl\", data=SEAA_XGB_feature_list, pickle_file=True, **aws_env_vars)\n",
    "    sf.write_to_s3(file_path=f\"{features_s3_path}/SEAA_LGBM_feat_list.pkl\", data=SEAA_LGBM_feature_list, pickle_file=True, **aws_env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Feature Selection DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes approximately 30 hours to execute on a 32 core x 5GHz machine\n",
    "# GLOB_XGB_feature_df = feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='xgb')\n",
    "# GLOB_LGBM_feature_df = feature_model(Xtt=CARB_Xtt, Xvt=CARB_Xvt, Xht=CARB_Xht, ytt=CARB_ytt, yvt=CARB_yvt, yht=CARB_yht, feature=False, model_type='lgbm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if writing out data\n",
    "# if deepnote:\n",
    "#     # Deep Note Local\n",
    "#     GLOB_XGB_feature_df.to_parquet(f\"{features_dn_path}/XGBoost_feature_selection_GLOB.parquet\")\n",
    "#     GLOB_LGBM_feature_df.to_parquet(f\"{features_dn_path}/LightGBM_feature_selection_GLOB.parquet\")\n",
    "\n",
    "#     # S3 Integration\n",
    "#     GLOB_XGB_feature_df.to_parquet(f\"{features_dns3_path}/XGBoost_feature_selection_GLOB.parquet\")\n",
    "#     GLOB_LGBM_feature_df.to_parquet(f\"{features_dns3_path}/LightGBM_feature_selection_GLOB.parquet\")\n",
    "\n",
    "# else:\n",
    "#     sf.write_to_s3(file_path=f\"{features_s3_path}/XGBoost_feature_selection_GLOB.parquet\", data=GLOB_XGB_feature_df, **aws_env_vars)\n",
    "#     sf.write_to_s3(file_path=f\"{features_s3_path}/LightGBM_feature_selection_GLOB.parquet\", data=GLOB_LGBM_feature_df, **aws_env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read DataFrames from disk if available instead of building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    GLOB_XGB_feature_df = pd.read_parquet(f\"{features_dn_path}/XGBoost_feature_selection_GLOB.parquet\")\n",
    "    GLOB_LGBM_feature_df = pd.read_parquet(f\"{features_dn_path}/LightGBM_feature_selection_GLOB.parquet\")\n",
    "else:\n",
    "    GLOB_XGB_feature_df = pd.read_parquet(sf.load_from_s3(file_path=f\"{features_s3_path}/XGBoost_feature_selection_GLOB.parquet\", **aws_env_vars))\n",
    "    GLOB_LGBM_feature_df = pd.read_parquet(sf.load_from_s3(file_path=f\"{features_s3_path}/LightGBM_feature_selection_GLOB.parquet\", **aws_env_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 features that were tested where the \"Val_MAE_Change\" is greater than or equal to 0\n",
    "# This will show the best combination of features that produce the lowest MAE score\n",
    "GLOB_XGB_feature_df[GLOB_XGB_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 10 features that were tested where the \"Val_MAE_Change\" is greater than or equal to 0\n",
    "# Count the number of features that were used to generate that MAE score\n",
    "GLOB_XGB_feature_df.sort_values(by=['Val_MAE'], ascending=True).head(10)['Features'].str.count(\",\") + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write out top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOB_XGB_feature_list = GLOB_XGB_feature_df[GLOB_XGB_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(1)['Features'].str.split(',').to_list()\n",
    "GLOB_XGB_feature_list = GLOB_XGB_feature_list[0]\n",
    "\n",
    "GLOB_LGBM_feature_list = GLOB_LGBM_feature_df[GLOB_LGBM_feature_df['Val_MAE_Change'] >= 0].sort_values(by=['Val_MAE'], ascending=True).head(1)['Features'].str.split(',').to_list()\n",
    "GLOB_LGBM_feature_list = GLOB_LGBM_feature_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    with open(f'{features_dn_path}GLOB_XGB_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(GLOB_XGB_feature_list, f)\n",
    "    with open(f'{features_dn_path}GLOB_LGBM_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(GLOB_LGBM_feature_list, f)\n",
    "    with open(f'{features_dns3_path}GLOB_XGB_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(GLOB_XGB_feature_list, f)\n",
    "    with open(f'{features_dns3_path}GLOB_LGBM_feat_list.pkl', 'wb') as f:\n",
    "                    pickle.dump(GLOB_LGBM_feature_list, f)\n",
    "else:\n",
    "    sf.write_to_s3(file_path=f\"{features_s3_path}/GLOB_XGB_feat_list.pkl\", data=GLOB_XGB_feature_list, pickle_file=True, **aws_env_vars)\n",
    "    sf.write_to_s3(file_path=f\"{features_s3_path}/GLOB_LGBM_feat_list.pkl\", data=GLOB_LGBM_feature_list, pickle_file=True, **aws_env_vars)"
   ]
  }
 ],
 "metadata": {
  "deepnote_app_hide_all_code_blocks_enabled": false,
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a0b2842f31e24974a3bd6a0797c1a6ae",
  "deepnote_persisted_session": {
   "createdAt": "2024-04-11T16:00:05.101Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
