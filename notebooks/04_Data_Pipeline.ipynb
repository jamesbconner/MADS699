{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "23d45ac132b648c5b9d9945088d01096",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aa5dd0d8a19d449b94c34a0ef6724c44",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "830451d1b74f4de196f69d1867ca559b",
    "deepnote_cell_type": "text-cell-h3",
    "formattedRanges": []
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-07T17:02:32.317230300Z",
     "start_time": "2024-04-07T17:02:31.079067700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Standard Libraries\n",
    "import os\n",
    "import datetime\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Visualization Libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import shap\n",
    "\n",
    "# Import custom functions\n",
    "import env_functions as ef\n",
    "import s3_functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T22:46:09.254859400Z",
     "start_time": "2024-04-06T22:46:09.252348700Z"
    }
   },
   "outputs": [],
   "source": [
    "# SKLearn Pipeline Imports\n",
    "from sklearn import set_config\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "#from sklearn.preprocessing import FunctionTransformer, PowerTransformer, RobustScaler, QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T22:46:09.796823300Z",
     "start_time": "2024-04-06T22:46:09.793730900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Multicolinearity Imports\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T22:46:10.693387600Z",
     "start_time": "2024-04-06T22:46:10.687612800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the environment and get appropriate vars\n",
    "deepnote, env_vars = ef.load_env_vars()\n",
    "\n",
    "# Iterate through the vars and set them as global vars\n",
    "for var_name, var in env_vars.items():\n",
    "    globals()[var_name] = var\n",
    "\n",
    "# If not in the DeepNote environment, create a dict for aws creds\n",
    "#   that were located in the environment file.  This will be passed\n",
    "#   to all aws s3 functions.\n",
    "if not deepnote:\n",
    "    aws_env_vars = {\n",
    "        'access_key_id': aws_access_key_id,\n",
    "        'secret_access_key': aws_secret_access_key,\n",
    "        'bucket_name': s3_bucket_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T22:46:12.631400Z",
     "start_time": "2024-04-06T22:46:12.628396800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas Configs\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=FutureWarning)\n",
    "warnings.simplefilter('ignore', category=UserWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# MapBox Token for Plotly Maps\n",
    "px.set_mapbox_access_token(os.environ.get(\"MAPBOX_TOKEN\"))\n",
    "\n",
    "# Scikit Learn Configs\n",
    "set_config(transform_output=\"pandas\")\n",
    "set_config(display=\"diagram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    # GLOB Data\n",
    "    X_train_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_train_GLOB.parquet\")\n",
    "    X_val_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_val_GLOB.parquet\")\n",
    "    X_holdout_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_holdout_GLOB.parquet\")\n",
    "    \n",
    "    y_train_GLOB = pd.read_parquet(\"/work/data/Xy_Data/y_train_GLOB.parquet\")\n",
    "    y_val_GLOB = pd.read_parquet(\"/work/data/Xy_Data/y_val_GLOB.parquet\")\n",
    "    y_holdout_GLOB = pd.read_parquet(\"/work/data/Xy_Data/y_holdout_GLOB.parquet\")\n",
    "    \n",
    "    # CARB Data\n",
    "    X_train_CARB = pd.read_parquet(\"/work/data/Xy_Data/X_train_CARB.parquet\")\n",
    "    X_val_CARB = pd.read_parquet(\"/work/data/Xy_Data/X_val_CARB.parquet\")\n",
    "    X_holdout_CARB = pd.read_parquet(\"/work/data/Xy_Data/X_holdout_CARB.parquet\")\n",
    "    \n",
    "    y_train_CARB = pd.read_parquet(\"/work/data/Xy_Data/y_train_CARB.parquet\")\n",
    "    y_val_CARB = pd.read_parquet(\"/work/data/Xy_Data/y_val_CARB.parquet\")\n",
    "    y_holdout_CARB = pd.read_parquet(\"/work/data/Xy_Data/y_holdout_CARB.parquet\")\n",
    "    \n",
    "    # SEAA Data\n",
    "    X_train_SEAA = pd.read_parquet(\"/work/data/Xy_Data/X_train_SEAA.parquet\")\n",
    "    X_val_SEAA = pd.read_parquet(\"/work/data/Xy_Data/X_val_SEAA.parquet\")\n",
    "    X_holdout_SEAA = pd.read_parquet(\"/work/data/Xy_Data/X_holdout_SEAA.parquet\")\n",
    "    \n",
    "    y_train_SEAA = pd.read_parquet(\"/work/data/Xy_Data/y_train_SEAA.parquet\")\n",
    "    y_val_SEAA = pd.read_parquet(\"/work/data/Xy_Data/y_val_SEAA.parquet\")\n",
    "    y_holdout_SEAA = pd.read_parquet(\"/work/data/Xy_Data/y_holdout_SEAA.parquet\")\n",
    "else:\n",
    "    # GLOB Data\n",
    "    X_train_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_train_GLOB.parquet\", **aws_env_vars))\n",
    "    X_val_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_val_GLOB.parquet\", **aws_env_vars))\n",
    "    X_holdout_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_holdout_GLOB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    y_train_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_train_GLOB.parquet\", **aws_env_vars))\n",
    "    y_val_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_val_GLOB.parquet\", **aws_env_vars))\n",
    "    y_holdout_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_holdout_GLOB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    # CARB Data\n",
    "    X_train_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_train_CARB.parquet\", **aws_env_vars))\n",
    "    X_val_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_val_CARB.parquet\", **aws_env_vars))\n",
    "    X_holdout_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_holdout_CARB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    y_train_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_train_CARB.parquet\", **aws_env_vars))\n",
    "    y_val_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_val_CARB.parquet\", **aws_env_vars))\n",
    "    y_holdout_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_holdout_CARB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    # SEAA Data\n",
    "    X_train_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_train_SEAA.parquet\", **aws_env_vars))\n",
    "    X_val_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_val_SEAA.parquet\", **aws_env_vars))\n",
    "    X_holdout_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_holdout_SEAA.parquet\", **aws_env_vars))\n",
    "    \n",
    "    y_train_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_train_SEAA.parquet\", **aws_env_vars))\n",
    "    y_val_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_val_SEAA.parquet\", **aws_env_vars))\n",
    "    y_holdout_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_holdout_SEAA.parquet\", **aws_env_vars))\n",
    "\n",
    "GLOB_X = [X_train_GLOB, X_val_GLOB, X_holdout_GLOB]\n",
    "GLOB_Y = [y_train_GLOB, y_val_GLOB, y_holdout_GLOB]\n",
    "\n",
    "SEAA_X = [X_train_SEAA, X_val_SEAA, X_holdout_SEAA]\n",
    "SEAA_Y = [y_train_SEAA, y_val_SEAA, y_holdout_SEAA]\n",
    "\n",
    "CARB_X = [X_train_CARB, X_val_CARB, X_holdout_CARB]\n",
    "CARB_Y = [y_train_CARB, y_val_CARB, y_holdout_CARB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the custom data transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "\n",
    "        ##########\n",
    "        # Define the deep copy so it leaves the original in place\n",
    "        Xc = X.copy(deep=True)\n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Define complex features based on Feature Importance and Interactions\n",
    "        epsilon = 0.0000001 # Avoid division by zero\n",
    "        Xc['Fishing_Inter'] = Xc['EN.FSH.THRD.NO'] * Xc['ER.FSH.CAPT.MT'] # Multiplicative effect of fishing and threatened species\n",
    "        Xc['Fertilizer_Inter'] = Xc['AG.LND.AGRI.K2'] / (Xc['AG.CON.FERT.ZS'] + epsilon) # Proportional effect of agricultural land and fertilizer\n",
    "        Xc['Fert_and_Turbidity_Interaction'] = Xc['Fertilizer_Inter'] * Xc['Turbidity']\n",
    "        Xc['Dist_to_Shore_and_Turbidity_Interaction'] = Xc['Distance_to_Shore'] * Xc['Turbidity']\n",
    "\n",
    "        # ##########\n",
    "        # Define Thermal Interaction Features \n",
    "        # Polynomial:\n",
    "        Xc['TSA_squared'] = Xc['TSA'] ** 2\n",
    "        Xc['SSTA_squared'] = Xc['SSTA'] ** 2\n",
    "        Xc['SSTA_cubed'] = Xc['SSTA'] ** 3\n",
    "        Xc['TSA_cubed'] = Xc['TSA'] ** 3        \n",
    "        Xc['TSA_cubed_SSTA'] = Xc['TSA_cubed'] * Xc['SSTA']\n",
    "        Xc['TSA_cubed_SSTA_squared'] = Xc['TSA_cubed'] * Xc['SSTA_squared']\n",
    "\n",
    "\n",
    "        # ########## \n",
    "        # Define columns to be kept\n",
    "        keep_cols = [\n",
    "            'Depth_m',\n",
    "            'Distance_to_Shore',\n",
    "            'Longitude_Degrees',\n",
    "            'Turbidity',\n",
    "            'Latitude_Degrees',\n",
    "            'ClimSST',\n",
    "            'Cyclone_Frequency',\n",
    "            'SSTA_Frequency',\n",
    "            'TSA_Frequency',\n",
    "            'Fertilizer_Inter',\n",
    "            'Fishing_Inter',\n",
    "            'Dist_to_Shore_and_Turbidity_Interaction',\n",
    "            'Fert_and_Turbidity_Interaction',\n",
    "            'TSA_cubed_SSTA',\n",
    "            'TSA_cubed_SSTA_squared',\n",
    "            'SSTA','TSA','SSTA_DHW','TSA_DHW'\n",
    "            ]             \n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Select columns\n",
    "        Xc = Xc[keep_cols]\n",
    "\n",
    "\n",
    "        ##########\n",
    "        # Prep for output\n",
    "        # Sort the column names so the modeling process doesn't complain\n",
    "        Xc = Xc.sort_index(axis=1)\n",
    "\n",
    "        \n",
    "        # Return the transformed dataframe\n",
    "        return Xc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the data transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(loc, X_list, y_list, scaled=False, path='/work/data/Xy_Data', s_path='/datasets/s3/data/Xy_Data', s3_path='data/Xy_Data'):\n",
    "    \n",
    "    # Choose which pipeline object to use\n",
    "    if scaled:\n",
    "        pre_pipe = Pipeline([\n",
    "            ('DataTransformer', DataTransformer()),\n",
    "            ('Scaler', StandardScaler()),\n",
    "        ])\n",
    "    else:\n",
    "        pre_pipe = Pipeline([\n",
    "            ('DataTransformer', DataTransformer()),\n",
    "            # ('Scaler', StandardScaler()),\n",
    "        ])\n",
    "    \n",
    "    X_train_trans = pre_pipe.fit_transform(X_list[0], y_list[0])\n",
    "    X_val_trans = pre_pipe.transform(X_list[1])\n",
    "    X_holdout_trans = pre_pipe.transform(X_list[2])\n",
    "\n",
    "    if deepnote:\n",
    "        # Local\n",
    "        X_train_trans.to_parquet(f\"{path}/X_train_trans_{loc}.parquet\")\n",
    "        X_val_trans.to_parquet(f\"{path}/X_val_trans_{loc}.parquet\")\n",
    "        X_holdout_trans.to_parquet(f\"{path}/X_holdout_trans_{loc}.parquet\")\n",
    "        \n",
    "        # S3\n",
    "        X_train_trans.to_parquet(f\"{s_path}/X_train_trans_{loc}.parquet\")\n",
    "        X_val_trans.to_parquet(f\"{s_path}/X_val_trans_{loc}.parquet\")\n",
    "        X_holdout_trans.to_parquet(f\"{s_path}/X_holdout_trans_{loc}.parquet\")\n",
    "\n",
    "    else:\n",
    "        sf.write_to_s3(file_path=f\"{s3_path}/X_train_trans_{loc}.parquet\", data=X_train_trans, **aws_env_vars)\n",
    "        sf.write_to_s3(file_path=f\"{s3_path}/X_val_trans_{loc}.parquet\", data=X_val_trans, **aws_env_vars)\n",
    "        sf.write_to_s3(file_path=f\"{s3_path}/X_holdout_trans_{loc}.parquet\", data=X_holdout_trans, **aws_env_vars)\n",
    "        \n",
    "\n",
    "    return f\"{loc} processed for {X_list}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to reprocess the data\n",
    "# process_data('GLOB', GLOB_X, GLOB_Y)\n",
    "# process_data('SEAA', SEAA_X, SEAA_Y)\n",
    "# process_data('CARB', CARB_X, CARB_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data for Correlation and VIF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    # Load GLOB Data\n",
    "    X_train_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_train_trans_GLOB.parquet\")\n",
    "    X_val_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_val_trans_GLOB.parquet\")\n",
    "    X_holdout_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_holdout_trans_GLOB.parquet\")\n",
    "\n",
    "else:\n",
    "    # Load GLOB Data\n",
    "    X_train_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_train_trans_GLOB.parquet\", **aws_env_vars))\n",
    "    X_val_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_val_trans_GLOB.parquet\", **aws_env_vars))\n",
    "    X_holdout_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_holdout_trans_GLOB.parquet\", **aws_env_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Pearson Correlation Matrix to review collinearity of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rehydrate the x and y dataframes for the correlation matrix\n",
    "df = pd.concat([X_train_trans, y_train_trans], axis=1)\n",
    "\n",
    "# Create the correlation matrix\n",
    "corr_matrix = df.corr(method='pearson')\n",
    "corr_mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "# Plot the matrix\n",
    "plt.figure(figsize=(25, 25))  # Adjust the figure size as needed\n",
    "sns.heatmap(corr_matrix, mask=corr_mask, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True, cbar_kws={\"shrink\": 0.8})\n",
    "\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform multicollinearity check using Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF needs an intercept term in the model, so add a constant to X_train_trans\n",
    "X_with_const = add_constant(X_train_trans)\n",
    "\n",
    "# Calculate VIF for each predictor\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_with_const.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_with_const.values, i) for i in range(len(X_with_const.columns))]\n",
    "\n",
    "# Print VIF for each feature in descending order, skipping the constant\n",
    "print(vif_data[1:].sort_values(by='VIF', ascending=False))"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "9fc3b5ad1b434e3aa34b2422856a1a63",
  "deepnote_persisted_session": {
   "createdAt": "2024-03-27T00:00:34.351Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
