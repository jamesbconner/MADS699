{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import boto3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Vis Imports\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Determine ecosystem and load appropriate variables\n",
    "if os.getenv('DEEPNOTE_RUNTIME_UUID'):\n",
    "    deepnote = True\n",
    "    print('Running on Deepnote with Env and S3 integrations, skipping dotenv')\n",
    "else:\n",
    "    deepnote = False\n",
    "    print('Loading dotenv file')\n",
    "\n",
    "    # Private file contains non-public variable configurations for local development.  Not loaded to github.\n",
    "    # variables.env can be populated with user specific API and Access keys and is empty by default.  Loaded to github.\n",
    "    private_vars_path = Path(\"../private_variables.env\")\n",
    "    var_path = Path(\"../variables.env\")\n",
    "    \n",
    "    # Use the private vars if exists, otherwise use the public vars file\n",
    "    env_path = private_vars_path if private_vars_path.exists() else var_path\n",
    "    \n",
    "    # Load the environment variables from the env path\n",
    "    load_dotenv(env_path)\n",
    "    \n",
    "    # Establish the variables\n",
    "    aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "    aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "    s3_bucket_name = os.getenv('S3_BUCKET_NAME')\n",
    "    neptune_project = os.getenv('NEPTUNE_PROJECT')\n",
    "    neptune_api_key = os.getenv('NEPTUNE_API_KEY')\n",
    "    mapbox_api_key = os.getenv('MAPBOX_API_KEY')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T01:01:51.351993900Z",
     "start_time": "2024-04-06T01:01:51.346758300Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def list_s3_contents(file_path, access_key_id=aws_access_key_id, secret_access_key=aws_secret_access_key, bucket_name=s3_bucket_name):\n",
    "    \"\"\"\n",
    "    List the contents of an S3 bucket path, prioritizing directories first, \n",
    "    then files in alphabetical order.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The S3 bucket path to list.\n",
    "        access_key_id (str): AWS access key ID.\n",
    "        secret_access_key (str): AWS secret access key.\n",
    "        bucket_name (str): Name of the S3 bucket.\n",
    "    \"\"\"\n",
    "    # Initialize a boto3 client\n",
    "    s3_client = boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)\n",
    "    \n",
    "    # Add a trailing slash if not present to properly emulate directory behavior\n",
    "    if not file_path.endswith('/'):\n",
    "        file_path += '/'\n",
    "    \n",
    "    # List objects in the specified path\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=file_path, Delimiter='/')\n",
    "    \n",
    "    # Collect directories (CommonPrefixes) and files\n",
    "    directories = [cp['Prefix'] for cp in response.get('CommonPrefixes', [])]\n",
    "    files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'] != file_path]\n",
    "    \n",
    "    # Sort directories and files separately\n",
    "    directories.sort()\n",
    "    files.sort()\n",
    "    \n",
    "    # Combine directories and files for listing\n",
    "    all_contents = directories + files\n",
    "    \n",
    "    # Print or return the sorted list\n",
    "    for item in all_contents:\n",
    "        print(item)\n",
    "    \n",
    "    return all_contents"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T14:56:34.934659600Z",
     "start_time": "2024-04-06T14:56:34.929762Z"
    }
   },
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_from_s3(file_path, access_key_id=aws_access_key_id, secret_access_key=aws_secret_access_key, bucket_name=s3_bucket_name):\n",
    "    '''\n",
    "    Download a file from the S3 bucket location\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path of the file within the S3 bucket.\n",
    "        access_key_id (str, optional): The AWS access key ID. Defaults to global aws_access_key_id variable.\n",
    "        secret_access_key (str, optional): The AWS secret access key. Defaults to global aws_secret_access_key variable.\n",
    "        bucket_name (str, optional): The name of the S3 bucket. Defaults to global s3_bucket_name variable.\n",
    "\n",
    "    Returns:\n",
    "        io.BytesIO: A BytesIO object containing the file content.\n",
    "    '''\n",
    "    \n",
    "    # Initialize a boto3 s3 client with credentials from the .env file\n",
    "    s3_client = boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)\n",
    "    \n",
    "    # Use the client to grab the data\n",
    "    f_obj = s3_client.get_object(Bucket=bucket_name, Key=file_path)\n",
    "    \n",
    "    # Set f to the body of the file object\n",
    "    f = io.BytesIO(f_obj['Body'].read())\n",
    "    \n",
    "    s3_client.close()\n",
    "        \n",
    "    return f\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T14:37:33.094576Z",
     "start_time": "2024-04-06T14:37:33.087918600Z"
    }
   },
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def write_to_s3(file_path, data, access_key_id=aws_access_key_id, secret_access_key=aws_secret_access_key, bucket_name=s3_bucket_name, pickle_file=False):\n",
    "    '''\n",
    "    Upload a file to the S3 bucket location\n",
    "    \n",
    "     Args:\n",
    "        file_path (str): The path to store the data within the S3 bucket.\n",
    "        data ([pd.DataFrame, Any]): The data to upload. Can be a Pandas DataFrame or any picklable object.\n",
    "        access_key_id (str, optional): The AWS access key ID. Defaults to global aws_access_key_id variable.\n",
    "        secret_access_key (str, optional): The AWS secret access key. Defaults to global aws_secret_access_key variable.\n",
    "        bucket_name (str, optional): The name of the S3 bucket. Defaults to global s3_bucket_name variable.\n",
    "        pickle_file (bool, optional): Whether to pickle the data before uploading. Defaults to False.\n",
    "    '''\n",
    "    \n",
    "    # Open the S3 client\n",
    "    s3_client = boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)\n",
    "    \n",
    "    if pickle_file:\n",
    "        pickled_data = pickle.dumps(data)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=file_path, Body=pickled_data)\n",
    "    \n",
    "    elif isinstance(data, pd.DataFrame):\n",
    "        with io.BytesIO() as buffer:\n",
    "            data.to_parquet(buffer)\n",
    "            buffer.seek(0)\n",
    "            s3_client.put_object(Bucket=bucket_name, Key=file_path, Body=buffer)\n",
    "        \n",
    "    else:\n",
    "        s3_client.close()\n",
    "        raise ValueError(\"Unsuppored data type for upload\")\n",
    "    \n",
    "    s3_client.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T14:37:49.613367400Z",
     "start_time": "2024-04-06T14:37:49.607972Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gcb_df = pd.read_csv(load_from_s3(file_path=\"data/Global_Coral_Bleaching_DB/Global_Coral_Bleaching_DB_v3.csv\"), low_memory=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T01:02:29.353980900Z",
     "start_time": "2024-04-06T01:02:26.359604900Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Impute Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Simple Imputation - Columns Missing Handfull of Values\n",
    "\n",
    "In this section we update some columns that were missing a couple of values (1-10) and we discard all rows that were missing the cruical column 'SSTA' which dramatically cleaned up the dataframe while only discarding about 0.4% of the entire dataframe. It should also be noted that rows missing 'SSTA' were also missing other cruical oceanic information."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Fix the observation missing the French Polynesian country name\n",
    "gcb_df.Country_Name.fillna(\"French Polynesia\", inplace=True)\n",
    "\n",
    "# Only 3 missing records for Ecoregion, all are on the northern coast of Honshu Island, Japan\n",
    "gcb_df.Ecoregion_Name.fillna(\"Honshu, Japan\", inplace=True)\n",
    "\n",
    "# If Substrate_Name is missing, it's hard coral\n",
    "gcb_df.Substrate_Name.fillna(\"Hard Coral\", inplace=True)\n",
    "\n",
    "# If Distance_to_Shore is missing, it's a FL Key site, 62m from shore\n",
    "gcb_df.Distance_to_Shore.fillna(62, inplace=True)\n",
    "\n",
    "# 'SSTA' is a column that seems to track with other important oceanic metrics.\n",
    "# There are 259 rows that are missing 'SSTA' and the dataset is about 63k rows.\n",
    "# This means that we are only discarding about 0.4% of the total dataframe while retaining\n",
    "# a bulk of the information that we need for a quality analysis.\n",
    "gcb_df.dropna(subset=['SSTA'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Not So Simple Imputation - Depth_m & SSTA_Minimum\n",
    "\n",
    "After performing the simple imputations above, we still have a couple of important columns that could be cleaned up: Depth_m and SSTA_Minimum."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def impute_depth(row):\n",
    "    '''\n",
    "    Function to impute Depth_m\n",
    "    param row: Pandas dataframe row object\n",
    "    return: depth\n",
    "    '''\n",
    "\n",
    "    # Set the default value to None\n",
    "    depth = None\n",
    "\n",
    "    # Assign the current value of depth as 'depth'\n",
    "    depth = row['Depth_m']\n",
    "\n",
    "    if not pd.isna(depth):\n",
    "        return depth\n",
    "    else:\n",
    "        temp_df = gcb_df[gcb_df['Reef_ID'] == row['Reef_ID']]\n",
    "        reef_median = temp_df['Depth_m'].median()\n",
    "\n",
    "        if not np.isnan(reef_median):\n",
    "            return reef_median\n",
    "        else:\n",
    "            # If the reef median is also NaN, use the overall median\n",
    "            overall_median = gcb_df['Depth_m'].median()\n",
    "            return overall_median"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-06T01:06:49.520154900Z",
     "start_time": "2024-04-06T01:06:49.514296200Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def impute_SSTAMin(row):\n",
    "    '''\n",
    "    Function to impute SSTA_Minimum\n",
    "    param row: Pandas dataframe row object\n",
    "    return: SSTA_Minimum\n",
    "    '''\n",
    "\n",
    "    # Set the default value to None\n",
    "    sstaValue = None\n",
    "\n",
    "    # Assign the current value of SSTA_Minimum as 'sstaValue'\n",
    "    sstaValue = row['SSTA_Minimum']\n",
    "\n",
    "    if not pd.isna(sstaValue):\n",
    "        return sstaValue\n",
    "    else:\n",
    "        temp_df = gcb_df[gcb_df['Ocean_Name'] == row['Ocean_Name']]\n",
    "        ocean_median = temp_df['SSTA_Minimum'].median()\n",
    "\n",
    "        if not np.isnan(ocean_median):\n",
    "            return ocean_median\n",
    "        else:\n",
    "            # If the ocean median is also NaN, use the overall median\n",
    "            overall_median = gcb_df['SSTA_Minimum'].median()\n",
    "            return overall_median"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gcb_df['Depth_m'] = gcb_df.apply(impute_depth, axis=1)\n",
    "gcb_df['SSTA_Minimum'] = gcb_df.apply(impute_SSTAMin, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Write Out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Uncomment to write out parquet file, which is used in the feature building notebook\n",
    "gcb_df.to_parquet(\"/work/data/Global_Coral_Bleaching_DB/gcb_v3.parquet\")\n",
    "gcb_df.to_parquet(\"/datasets/s3/data/Global_Coral_Bleaching_DB/gcb_v3.parquet\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Validate the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# How much data is missing from the columns?\n",
    "\n",
    "#gcb_df.isna().sum() # Total NaN per col\n",
    "#gcb_df.isna().mean() # Percent NaN per col\n",
    "\n",
    "# Number of observations with no bleaching value\n",
    "#len(gcb_df[gcb_df[\"Percent_Bleached_Value\"].isna()])\n",
    "\n",
    "# Identify the columns missing more than 10% of data\n",
    "gcb_cols = gcb_df.columns[gcb_df.isna().mean() > 0.1]\n",
    "gcb_df[gcb_cols].isna().mean()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_persisted_session": {
   "createdAt": "2024-03-27T00:00:34.646Z"
  },
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_hide_all_code_blocks_enabled": false,
  "deepnote_app_reactivity_enabled": true,
  "deepnote_notebook_id": "d0547b5227674239a1edf00a8adfc80e",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
