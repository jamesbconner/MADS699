{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleansing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup Notebook"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:34.116644900Z",
     "start_time": "2024-04-06T21:07:34.113132300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import custom functions\n",
    "import env_functions as ef\n",
    "import s3_functions as sf\n",
    "\n",
    "# Vis Imports\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:42.821185900Z",
     "start_time": "2024-04-06T21:07:42.815011100Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:42.825190Z",
     "start_time": "2024-04-06T21:07:42.816516900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dotenv file\n"
     ]
    }
   ],
   "source": [
    "# Determine the environment and get appropriate vars\n",
    "deepnote, env_vars = ef.load_env_vars()\n",
    "\n",
    "# Iterate through the vars and set them as global vars\n",
    "for var_name, var in env_vars.items():\n",
    "    globals()[var_name] = var\n",
    "\n",
    "# If not in the DeepNote environment, create a dict for aws creds\n",
    "#   that were located in the environment file.  This will be passed\n",
    "#   to all aws s3 functions.\n",
    "if not deepnote:\n",
    "    aws_env_vars = {\n",
    "        'access_key_id': aws_access_key_id,\n",
    "        'secret_access_key': aws_secret_access_key,\n",
    "        'bucket_name': s3_bucket_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:45.129242900Z",
     "start_time": "2024-04-06T21:07:42.823189400Z"
    }
   },
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    gcb_df = pd.read_csv(\"/datasets/s3/data/Global_Coral_Bleaching_DB/Global_Coral_Bleaching_DB_v3.csv\", low_memory=False)\n",
    "else:\n",
    "    gcb_df = pd.read_csv(sf.load_from_s3(file_path=\"data/Global_Coral_Bleaching_DB/Global_Coral_Bleaching_DB_v3.csv\", **aws_env_vars), low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Imputation - Columns Missing Handfull of Values\n",
    "\n",
    "In this section we update some columns that were missing a couple of values (1-10) and we discard all rows that were missing the cruical column 'SSTA' which dramatically cleaned up the dataframe while only discarding about 0.4% of the entire dataframe. It should also be noted that rows missing 'SSTA' were also missing other cruical oceanic information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:45.133249400Z",
     "start_time": "2024-04-06T21:07:45.087368600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fix the observation missing the French Polynesian country name\n",
    "gcb_df.Country_Name.fillna(\"French Polynesia\", inplace=True)\n",
    "\n",
    "# Only 3 missing records for Ecoregion, all are on the northern coast of Honshu Island, Japan\n",
    "gcb_df.Ecoregion_Name.fillna(\"Honshu, Japan\", inplace=True)\n",
    "\n",
    "# If Substrate_Name is missing, it's hard coral\n",
    "gcb_df.Substrate_Name.fillna(\"Hard Coral\", inplace=True)\n",
    "\n",
    "# If Distance_to_Shore is missing, it's a FL Key site, 62m from shore\n",
    "gcb_df.Distance_to_Shore.fillna(62, inplace=True)\n",
    "\n",
    "# 'SSTA' is a column that seems to track with other important oceanic metrics.\n",
    "# There are 259 rows that are missing 'SSTA' and the dataset is about 63k rows.\n",
    "# This means that we are only discarding about 0.4% of the total dataframe while retaining\n",
    "# a bulk of the information that we need for a quality analysis.\n",
    "gcb_df.dropna(subset=['SSTA'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not So Simple Imputation - Depth_m & SSTA_Minimum\n",
    "\n",
    "After performing the simple imputations above, we still have a couple of important columns that could be cleaned up: Depth_m and SSTA_Minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:45.134249700Z",
     "start_time": "2024-04-06T21:07:45.089181800Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_depth(row):\n",
    "    '''\n",
    "    Function to impute Depth_m\n",
    "    param row: Pandas dataframe row object\n",
    "    return: depth\n",
    "    '''\n",
    "\n",
    "    # Set the default value to None\n",
    "    depth = None\n",
    "\n",
    "    # Assign the current value of depth as 'depth'\n",
    "    depth = row['Depth_m']\n",
    "\n",
    "    if not pd.isna(depth):\n",
    "        return depth\n",
    "    else:\n",
    "        temp_df = gcb_df[gcb_df['Reef_ID'] == row['Reef_ID']]\n",
    "        reef_median = temp_df['Depth_m'].median()\n",
    "\n",
    "        if not np.isnan(reef_median):\n",
    "            return reef_median\n",
    "        else:\n",
    "            # If the reef median is also NaN, use the overall median\n",
    "            overall_median = gcb_df['Depth_m'].median()\n",
    "            return overall_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:53.808833800Z",
     "start_time": "2024-04-06T21:07:53.805363600Z"
    }
   },
   "outputs": [],
   "source": [
    "def impute_SSTAMin(row):\n",
    "    '''\n",
    "    Function to impute SSTA_Minimum\n",
    "    param row: Pandas dataframe row object\n",
    "    return: SSTA_Minimum\n",
    "    '''\n",
    "\n",
    "    # Set the default value to None\n",
    "    sstaValue = None\n",
    "\n",
    "    # Assign the current value of SSTA_Minimum as 'sstaValue'\n",
    "    sstaValue = row['SSTA_Minimum']\n",
    "\n",
    "    if not pd.isna(sstaValue):\n",
    "        return sstaValue\n",
    "    else:\n",
    "        temp_df = gcb_df[gcb_df['Ocean_Name'] == row['Ocean_Name']]\n",
    "        ocean_median = temp_df['SSTA_Minimum'].median()\n",
    "\n",
    "        if not np.isnan(ocean_median):\n",
    "            return ocean_median\n",
    "        else:\n",
    "            # If the ocean median is also NaN, use the overall median\n",
    "            overall_median = gcb_df['SSTA_Minimum'].median()\n",
    "            return overall_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:55.416498600Z",
     "start_time": "2024-04-06T21:07:53.808833800Z"
    }
   },
   "outputs": [],
   "source": [
    "gcb_df['Depth_m'] = gcb_df.apply(impute_depth, axis=1)\n",
    "gcb_df['SSTA_Minimum'] = gcb_df.apply(impute_SSTAMin, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:55.420004700Z",
     "start_time": "2024-04-06T21:07:55.417498800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment to write out parquet file, which is used in the feature building notebook\n",
    "# if deepnote:\n",
    "#     gcb_df.to_parquet(\"/work/data/Global_Coral_Bleaching_DB/gcb_v3.parquet\")\n",
    "#     gcb_df.to_parquet(\"/datasets/s3/data/Global_Coral_Bleaching_DB/gcb_v3.parquet\")\n",
    "# else:\n",
    "#     write_to_s3(file_path=\"data/Global_Coral_Bleaching_DB/gcb_v3.parquet\", data=gcb_df, **aws_env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T21:07:55.458766Z",
     "start_time": "2024-04-06T21:07:55.421007900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reef_ID                         0.20\n",
       "Bleaching_Level                 0.30\n",
       "Percent_Bleached                0.86\n",
       "Percent_Bleaching_Old_Method    1.00\n",
       "S1                              0.30\n",
       "S2                              0.30\n",
       "S3                              0.30\n",
       "S4                              0.31\n",
       "Bleaching_Prevalence_Score      1.00\n",
       "Bleaching_Prevalence_Score_ID   1.00\n",
       "Severity_Code                   0.89\n",
       "Severity_ID                     0.89\n",
       "bleach_intensity                1.00\n",
       "Number_Bleached_Colonies        1.00\n",
       "Percent_Hard_Coral              1.00\n",
       "Percent_Macroalgae              1.00\n",
       "Site_Name                       0.87\n",
       "City_Town_Name_2                0.40\n",
       "City_Town_Name_3                0.87\n",
       "City_Town_Name_4                1.00\n",
       "Sample_Comments                 0.95\n",
       "Site Comments                   0.96\n",
       "Cover Comments                  1.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much data is missing from the columns?\n",
    "\n",
    "#gcb_df.isna().sum() # Total NaN per col\n",
    "#gcb_df.isna().mean() # Percent NaN per col\n",
    "\n",
    "# Number of observations with no bleaching value\n",
    "#len(gcb_df[gcb_df[\"Percent_Bleached_Value\"].isna()])\n",
    "\n",
    "# Identify the columns missing more than 10% of data\n",
    "gcb_cols = gcb_df.columns[gcb_df.isna().mean() > 0.1]\n",
    "gcb_df[gcb_cols].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_app_hide_all_code_blocks_enabled": false,
  "deepnote_app_layout": "powerful-article",
  "deepnote_app_reactivity_enabled": true,
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "d0547b5227674239a1edf00a8adfc80e",
  "deepnote_persisted_session": {
   "createdAt": "2024-03-27T00:00:34.646Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
