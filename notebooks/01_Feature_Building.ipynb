{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "666676dfa9a747bc8dfb1eb62393151c",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Feature Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "7de3ada45d104d0f8d66a9046a79d67c",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Import custom functions\n",
    "import env_functions as ef\n",
    "import s3_functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', category=FutureWarning)\n",
    "warnings.simplefilter('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the environment and get appropriate vars\n",
    "deepnote, env_vars = ef.load_env_vars()\n",
    "\n",
    "# Iterate through the vars and set them as global vars\n",
    "for var_name, var in env_vars.items():\n",
    "    globals()[var_name] = var\n",
    "\n",
    "# If not in the DeepNote environment, create a dict for aws creds\n",
    "#   that were located in the environment file.  This will be passed\n",
    "#   to all aws s3 functions.\n",
    "if not deepnote:\n",
    "    aws_env_vars = {\n",
    "        'access_key_id': aws_access_key_id,\n",
    "        'secret_access_key': aws_secret_access_key,\n",
    "        'bucket_name': s3_bucket_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T20:50:46.848224900Z",
     "start_time": "2024-04-06T20:50:38.600878500Z"
    }
   },
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    gcb = pd.read_parquet(\"/work/data/Global_Coral_Bleaching_DB/gcb_v3.parquet\")\n",
    "    country_code_df = pd.read_csv(\"/work/data/References/country_name_codes.csv\")\n",
    "    meow = gpd.read_file(\"/work/data/MEOW/meow_ecos.shp\")\n",
    "    wdi = pd.read_csv(\"/work/data/World_Bank/wdi_1980_to_2020.csv\")\n",
    "else:\n",
    "    gcb = pd.read_parquet(sf.load_from_s3(file_path=\"data/Global_Coral_Bleaching_DB/gcb_v3.parquet\", **aws_env_vars))\n",
    "    country_code_df = pd.read_csv(sf.load_from_s3(file_path=\"data/References/country_name_codes.csv\", **aws_env_vars), low_memory=False)\n",
    "    meow = gpd.read_file(sf.load_from_s3(file_path=\"data/MEOW/meow_ecos.zip\", **aws_env_vars))\n",
    "    wdi = pd.read_csv(sf.load_from_s3(file_path=\"data/World_Bank/wdi_1980_to_2020.csv\", **aws_env_vars), low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Percent_Bleached_Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_bleached_calc(row):\n",
    "    '''\n",
    "    Function to determine Percent_Bleached_Value\n",
    "    param row: Pandas dataframe row object\n",
    "    return: Percent Bleached\n",
    "    '''\n",
    "\n",
    "    # Set the default value to None\n",
    "    bleach_val = None\n",
    "\n",
    "    # Create a dict for the Donner severity codes\n",
    "    # These values are currently hard coded to the mean of the range\n",
    "    #   but could be imputed more programmatically using a distribution\n",
    "    severity_pcts = {-1:0, 0:0, 1:5, 2:30, 3:75}\n",
    "\n",
    "    # Create a dict for the Safaie prevalence score\n",
    "    # These values are currently hard coded to the mean of the range\n",
    "    #   but could be imputed more programmatically using a distribution\n",
    "    prevalence_pcts = {0:0, 1:5, 2:17.5, 3:37.5, 4:75}\n",
    "\n",
    "    # Reef Check uses sectors ... get a mean score\n",
    "    quad_avg = row[['S1','S2','S3','S4']].mean()\n",
    "\n",
    "    # Donner & Safaie use a severity code to indicate a general amount of bleaching\n",
    "    severity_est = severity_pcts.get(row['Severity_ID'], np.nan)\n",
    "    prevalence_est = prevalence_pcts.get(row['Bleaching_Prevalence_Score_ID'])\n",
    "\n",
    "    # Grab the Percent Bleached value\n",
    "    pct_bleached = row['Percent_Bleached']\n",
    "\n",
    "    # In order of preference for use:\n",
    "    # Percent_Bleached, the Quad Average, then Severity and Prevalence guesstimates\n",
    "    if not pd.isna(pct_bleached):\n",
    "        bleach_val = pct_bleached\n",
    "    elif not pd.isna(quad_avg):\n",
    "        bleach_val = quad_avg\n",
    "    elif not pd.isna(severity_est):\n",
    "        bleach_val = severity_est\n",
    "    elif not pd.isna(prevalence_est):\n",
    "        bleach_val = prevalence_est\n",
    "    else:\n",
    "        bleach_val = np.nan\n",
    "    \n",
    "    #print(bleach_val, pct_bleached, quad_avg, severity_est, prevalance_est)\n",
    "\n",
    "    # If the value is still nan (ie, no bleaching data available), return the nan\n",
    "    # Else look at the value and if it's smaller than 1, multiply by 100 and return\n",
    "    if pd.isna(bleach_val):\n",
    "        return bleach_val\n",
    "    elif bleach_val < 1:\n",
    "        bleach_val = bleach_val * 100\n",
    "        if bleach_val >= 1:\n",
    "            return bleach_val\n",
    "        else:\n",
    "            return bleach_val * 100\n",
    "    else:\n",
    "        return bleach_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Percent_Bleached_Value\n",
    "gcb['Percent_Bleached_Value'] = gcb.apply(pct_bleached_calc, axis=1)\n",
    "\n",
    "# Indicated which rows had the Percent_Bleached_Value imputed\n",
    "gcb['Bleached_Value_Imputed'] = (~gcb[\"Percent_Bleached_Value\"].isna()) & \\\n",
    "                                   (gcb['Percent_Bleached'].isna()) & \\\n",
    "                                   (gcb['S1'].isna()) & (gcb['S2'].isna()) & \\\n",
    "                                   (gcb['S3'].isna()) & (gcb['S4'].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Percent_Bleached_Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show records where Percent_Bleached_Value between 0 and 1\n",
    "# gcb[(gcb.Percent_Bleached_Value > 0) & (gcb.Percent_Bleached_Value < 1)]\n",
    "\n",
    "# Count number of records where Percent_Bleached_Value == 0\n",
    "len(gcb[gcb.Percent_Bleached_Value == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of those 0% Percent_Bleached_Values were before the year 2010?\n",
    "print(\"Total 0% Records Prior to 2010: \", len(gcb[(gcb.Percent_Bleached_Value == 0) & (gcb.Date_Year < 2010)]))\n",
    "print(\"Percentage of 0% Records Prior to 2010: \", len(gcb[(gcb.Percent_Bleached_Value == 0) & (gcb.Date_Year < 2010)])/len(gcb.Date_Year < 2010)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of those 0% Percent_Bleached_Values were after the year 2010?\n",
    "print(\"Total 0% Records After 2010: \", len(gcb[(gcb.Percent_Bleached_Value == 0) & (gcb.Date_Year >= 2010)]))\n",
    "print(\"Percentage of 0% Records After 2010: \", len(gcb[(gcb.Percent_Bleached_Value == 0) & (gcb.Date_Year >= 2010)])/len(gcb.Date_Year >= 2010)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a composite date column from the day/month/year columns\n",
    "gcb['Date'] = pd.to_datetime({\n",
    "    'year': gcb['Date_Year'],\n",
    "    'month': gcb['Date_Month'],\n",
    "    'day': gcb['Date_Day']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create short month names\n",
    "month_short_names = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "                    7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
    "gcb['Month_Name'] = gcb['Date_Month'].map(month_short_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datetime object at a Monthly grain\n",
    "gcb['Month_Year'] = gcb['Date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a country code variable for easy joining with other datasets\n",
    "gcb = pd.merge(\n",
    "    gcb, country_code_df[['Country_Name','Country_Code']], \n",
    "    how=\"left\", left_on=\"Country_Name\", right_on=\"Country_Name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal Features from Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Nominal Features from object dtypes\n",
    "gcb['Exposure_Cat'] = gcb['Exposure'].astype('category').cat.codes\n",
    "gcb['Country_Name_Cat'] = gcb['Country_Name'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MEOW Province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the dataframes as geopandas assets\n",
    "# Create a point geometry for the gcb dataset\n",
    "gdf_primary = gpd.GeoDataFrame(gcb, geometry=gcb.apply(lambda row: Point(row['Longitude_Degrees'], row['Latitude_Degrees']), axis=1))\n",
    "gdf_secondary = gpd.GeoDataFrame(meow, geometry=meow['geometry'])\n",
    "\n",
    "# Perform a spatial join using a 'within' operation and drop the right join index\n",
    "joined_gdf = gpd.sjoin(gdf_primary, gdf_secondary[['PROVINCE', 'geometry']], how='left', op='within').drop(columns=['index_right','geometry'])\n",
    "\n",
    "# Convert the resulting dataframe back to a standard pandas df\n",
    "# No need to drop the geometry column at this time\n",
    "gcb = pd.DataFrame(joined_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, there are 2 Southeast Polynesia datapoints \n",
    "#   that don't quite fall within the polygon shapes.  Impute them.\n",
    "gcb.PROVINCE.fillna('Southeast Polynesia', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add World Development Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the WDI dataset on the following WDI Codes\n",
    "# Fishing, Timber, Forest, Marine, Agriculture, Fertilizer, Tourism, and Pop Totals\n",
    "filter_codes = [\n",
    "    'AG.LND.IRIG.AG.ZS','AG.LND.AGRI.ZS','AG.LND.AGRI.K2',\n",
    "    'NV.AGR.TOTL.ZS','NV.AGR.TOTL.KD.ZG','NV.AGR.TOTL.CD',\n",
    "    'ER.FSH.AQUA.MT','ER.FSH.CAPT.MT','AG.CON.FERT.ZS',\n",
    "    'EN.FSH.THRD.NO','AG.LND.FRST.ZS','AG.LND.FRST.K2',\n",
    "    'ST.INT.ARVL','ER.MRN.PTMR.ZS','EN.POP.DNST',\n",
    "    'SP.POP.TOTL','ER.PTD.TOTL.ZS','ER.FSH.PROD.MT'\n",
    "    ]\n",
    "wdi_filtered = wdi[wdi['Series_Code'].isin(filter_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing WDI Values\n",
    "\n",
    "# Define year columns\n",
    "columns_years = [str(year) for year in range(1980, 2021)]\n",
    "\n",
    "# Impute AG.LND.IRIG.AG.ZS where totally missing\n",
    "map_fert_pct = {\n",
    "               'ATG': 7.69, 'BHS': 8.33, 'BRB': 39.00, 'BLZ': 3.48, 'BMU': 0, \n",
    "               'KHM': 9.17, 'CYM': 7.69, 'CHN': 51.48, 'COM': 0.13, 'CRI': 18.14,\n",
    "               'CUB': 15.68, 'DJI': 100, 'DMA': 0.87, 'EGY': 100, 'ERI': 5.49,\n",
    "               'FJI': 1.59, 'PYF': 3.88, 'IDN': 41.54, 'KEN': 2.34, 'KIR': 0,\n",
    "               'KWT': 67.6, 'MDV': 0, 'MHL': 0, 'FSM': 0, 'NCL': 0, 'NIC': 10.89,\n",
    "               'MNP': 0, 'PLW': 0, 'PNG': 0, 'WSM': 0, 'STP': 9.7, 'SGP': 0,\n",
    "               'SLB': 0.04, 'LKA': 29.23, 'KNA': 0.49, 'LCA': 32.26,\n",
    "               'VCT': 5.98, 'TZA': 2.32, 'THA': 33.76, 'TON': 0, 'TCA': 0, \n",
    "               'TUV': 0, 'VUT': 0, 'VNM': 48.67, 'YEM': 41.8\n",
    "               }\n",
    "\n",
    "wdi_filtered['temp_fert_pct'] = wdi_filtered['Country_Code'].map(map_fert_pct)\n",
    "for col in columns_years:\n",
    "    wdi_filtered[col] = wdi_filtered[col].fillna(wdi_filtered['temp_fert_pct'])\n",
    "wdi_filtered.drop('temp_fert_pct', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Impute AG.CON.FERT.ZS where totally missing\n",
    "map_fert_kgha = {'TCA': 0, 'TUV':0}\n",
    "\n",
    "wdi_filtered['map_fert_kgha'] = wdi_filtered['Country_Code'].map(map_fert_kgha)\n",
    "for col in columns_years:\n",
    "    wdi_filtered[col] = wdi_filtered[col].fillna(wdi_filtered['map_fert_kgha'])\n",
    "wdi_filtered.drop('map_fert_kgha', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Impute All codes where partially missing\n",
    "wdi_filtered['mean_1980_2000'] = wdi_filtered[columns_years].mean(axis=1)\n",
    "\n",
    "for column in columns_years:\n",
    "    wdi_filtered[column] = wdi_filtered.apply(lambda row: row['mean_1980_2000'] if pd.isna(row[column]) else row[column], axis=1)\n",
    "\n",
    "wdi_filtered.drop('mean_1980_2000', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_filtered.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataset so that Country_Code and Series_Code remain cols,\n",
    "#   while the years go from being columns to rows, with the Series_Code\n",
    "#   value going into the Value col\n",
    "melted_df = pd.melt(wdi_filtered, id_vars=['Country_Code', 'Series_Code'], \n",
    "                    value_vars=[str(year) for year in range(1980, 2021)],\n",
    "                    var_name='Year', value_name='Value')\n",
    "\n",
    "# Now pivot the dataset on the Series_Code column, so that each code\n",
    "#   becomes its own column.  Convert the Year to float for easy joining\n",
    "#   to the coral database dataframe\n",
    "pivoted_df = melted_df.pivot_table(index=['Country_Code', 'Year'], \n",
    "                                   columns='Series_Code', \n",
    "                                   values='Value', \n",
    "                                   aggfunc='first').reset_index()\n",
    "pivoted_df.Year = pivoted_df.Year.astype('float64')\n",
    "\n",
    "# Merge the coral database and the pivoted wdi dataframe\n",
    "gcb = pd.merge(gcb, pivoted_df, left_on=['Country_Code','Date_Year'], right_on=['Country_Code','Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdi_filtered.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df[pivoted_df.Country_Code == 'MDV'].sample(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcb.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine the Cols and Rows to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that will not provide value to the analysis\n",
    "gcb_drop_cols = [\n",
    "    'Percent_Bleaching_Old_Method', \n",
    "    'bleach_intensity', \n",
    "    'Number_Bleached_Colonies', \n",
    "    'Percent_Hard_Coral', \n",
    "    'Percent_Macroalgae',\n",
    "    'Sample_Method', \n",
    "    'Sample_Comments',\n",
    "    'Site Comments',\n",
    "    'Cover Comments', \n",
    "    'Site_Name'\n",
    "    ]\n",
    "\n",
    "gcb.drop(columns=gcb_drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where Percent_Bleached_Value is not a number greater than or equal to 0\n",
    "gcb = gcb[gcb.Percent_Bleached_Value >= 0]\n",
    "\n",
    "# Drop rows where the SSTA records are null\n",
    "gcb = gcb[gcb['SSTA'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Out Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out this version to a parquet file\n",
    "# if deepnote:\n",
    "    # gcb.to_parquet(\"/work/data/Global_Coral_Bleaching_DB/gcb_v4.parquet\")\n",
    "    # gcb.to_parquet(\"/datasets/s3/data/Global_Coral_Bleaching_DB/gcb_v4.parquet\")\n",
    "# else:\n",
    "    # write_to_s3(file_path=\"data/Global_Coral_Bleaching_DB/gcb_v4.parquet\", data=gcb, **aws_env_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Classification y-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-1, 0, 10, 50, 100]\n",
    "names = ['No Bleaching', 'Mild Bleaching', 'Moderate Bleaching', 'Severe Bleaching']\n",
    "\n",
    "gcb['Bleached_Class'] = pd.cut(gcb['Percent_Bleached_Value'], bins, labels=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the final parquet file, which is used in the analysis notebooks\n",
    "# if deepnote:\n",
    "#     gcb.to_parquet(\"/work/data/Global_Coral_Bleaching_DB/gcb_v5.parquet\")\n",
    "#     gcb.to_parquet(\"/datasets/s3/data/Global_Coral_Bleaching_DB/gcb_v5.parquet\")\n",
    "# else:\n",
    "#     write_to_s3(file_path=\"data/Global_Coral_Bleaching_DB/gcb_v5.parquet\", data=gcb, **aws_env_vars)"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "149240d282e943f0baf864bf605c0480",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
