{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notebook Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T22:41:23.329189100Z",
     "start_time": "2024-04-06T22:41:23.208238100Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "# Import custom functions\n",
    "import env_functions as ef\n",
    "import s3_functions as sf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T22:41:24.204635300Z",
     "start_time": "2024-04-06T22:41:24.198645200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the environment and get appropriate vars\n",
    "deepnote, env_vars = ef.load_env_vars()\n",
    "\n",
    "# Iterate through the vars and set them as global vars\n",
    "for var_name, var in env_vars.items():\n",
    "    globals()[var_name] = var\n",
    "\n",
    "# If not in the DeepNote environment, create a dict for aws creds\n",
    "#   that were located in the environment file.  This will be passed\n",
    "#   to all aws s3 functions.\n",
    "if not deepnote:\n",
    "    aws_env_vars = {\n",
    "        'access_key_id': aws_access_key_id,\n",
    "        'secret_access_key': aws_secret_access_key,\n",
    "        'bucket_name': s3_bucket_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import and Filter Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-06T22:41:36.418846200Z",
     "start_time": "2024-04-06T22:41:27.454713800Z"
    }
   },
   "outputs": [],
   "source": [
    "if deepnote:\n",
    "    # GLOB Data\n",
    "    X_train_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_train_GLOB.parquet\")\n",
    "    X_val_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_val_GLOB.parquet\")\n",
    "    X_holdout_GLOB = pd.read_parquet(\"/work/data/Xy_Data/X_holdout_GLOB.parquet\")\n",
    "    \n",
    "    y_train_GLOB = pd.read_parquet(\"/work/data/Xy_Data/y_train_GLOB.parquet\")\n",
    "    y_val_GLOB = pd.read_parquet(\"/work/data/Xy_Data/y_val_GLOB.parquet\")\n",
    "    y_holdout_GLOB = pd.read_parquet(\"/work/data/Xy_Data/y_holdout_GLOB.parquet\")\n",
    "    \n",
    "    # CARB Data\n",
    "    X_train_CARB = pd.read_parquet(\"/work/data/Xy_Data/X_train_CARB.parquet\")\n",
    "    X_val_CARB = pd.read_parquet(\"/work/data/Xy_Data/X_val_CARB.parquet\")\n",
    "    X_holdout_CARB = pd.read_parquet(\"/work/data/Xy_Data/X_holdout_CARB.parquet\")\n",
    "    \n",
    "    y_train_CARB = pd.read_parquet(\"/work/data/Xy_Data/y_train_CARB.parquet\")\n",
    "    y_val_CARB = pd.read_parquet(\"/work/data/Xy_Data/y_val_CARB.parquet\")\n",
    "    y_holdout_CARB = pd.read_parquet(\"/work/data/Xy_Data/y_holdout_CARB.parquet\")\n",
    "    \n",
    "    # SEAA Data\n",
    "    X_train_SEAA = pd.read_parquet(\"/work/data/Xy_Data/X_train_SEAA.parquet\")\n",
    "    X_val_SEAA = pd.read_parquet(\"/work/data/Xy_Data/X_val_SEAA.parquet\")\n",
    "    X_holdout_SEAA = pd.read_parquet(\"/work/data/Xy_Data/X_holdout_SEAA.parquet\")\n",
    "    \n",
    "    y_train_SEAA = pd.read_parquet(\"/work/data/Xy_Data/y_train_SEAA.parquet\")\n",
    "    y_val_SEAA = pd.read_parquet(\"/work/data/Xy_Data/y_val_SEAA.parquet\")\n",
    "    y_holdout_SEAA = pd.read_parquet(\"/work/data/Xy_Data/y_holdout_SEAA.parquet\")\n",
    "else:\n",
    "    # GLOB Data\n",
    "    X_train_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_train_GLOB.parquet\", **aws_env_vars))\n",
    "    X_val_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_val_GLOB.parquet\", **aws_env_vars))\n",
    "    X_holdout_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_holdout_GLOB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    y_train_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_train_GLOB.parquet\", **aws_env_vars))\n",
    "    y_val_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_val_GLOB.parquet\", **aws_env_vars))\n",
    "    y_holdout_GLOB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_holdout_GLOB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    # CARB Data\n",
    "    X_train_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_train_CARB.parquet\", **aws_env_vars))\n",
    "    X_val_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_val_CARB.parquet\", **aws_env_vars))\n",
    "    X_holdout_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_holdout_CARB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    y_train_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_train_CARB.parquet\", **aws_env_vars))\n",
    "    y_val_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_val_CARB.parquet\", **aws_env_vars))\n",
    "    y_holdout_CARB = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_holdout_CARB.parquet\", **aws_env_vars))\n",
    "    \n",
    "    # SEAA Data\n",
    "    X_train_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_train_SEAA.parquet\", **aws_env_vars))\n",
    "    X_val_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_val_SEAA.parquet\", **aws_env_vars))\n",
    "    X_holdout_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/X_holdout_SEAA.parquet\", **aws_env_vars))\n",
    "    \n",
    "    y_train_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_train_SEAA.parquet\", **aws_env_vars))\n",
    "    y_val_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_val_SEAA.parquet\", **aws_env_vars))\n",
    "    y_holdout_SEAA = pd.read_parquet(sf.load_from_s3(file_path=\"data/Xy_Data/y_holdout_SEAA.parquet\", **aws_env_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Sample_ID', 'Site_ID', 'Reef_ID', 'Date_Day', 'Date_Month',\n",
    "             'Date_Year', 'Ocean_Name','Realm_Name',\n",
    "             'Ecoregion_Name', 'Country_Name', 'State_Island_Province_Name',\n",
    "             'City_Town_Name_1', 'City_Town_Name_2', 'City_Town_Name_3',\n",
    "             'City_Town_Name_4', 'Data_Source', 'Bleached_Value_Imputed', 'Date',\n",
    "             'Month_Name', 'Month_Year', 'Country_Code', 'Exposure_Cat',\n",
    "             'Country_Name_Cat', 'PROVINCE', 'Year', 'Exposure', 'Substrate_Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_GLOB = X_train_GLOB.drop(columns=drop_cols)\n",
    "X_val_GLOB = X_val_GLOB.drop(columns=drop_cols)\n",
    "X_holdout_GLOB = X_holdout_GLOB.drop(columns=drop_cols)\n",
    "\n",
    "X_train_CARB = X_train_CARB.drop(columns=drop_cols)\n",
    "X_val_CARB = X_val_CARB.drop(columns=drop_cols)\n",
    "X_holdout_CARB = X_holdout_CARB.drop(columns=drop_cols)\n",
    "\n",
    "X_train_SEAA = X_train_SEAA.drop(columns=drop_cols)\n",
    "X_val_SEAA = X_val_SEAA.drop(columns=drop_cols)\n",
    "X_holdout_SEAA = X_holdout_SEAA.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis (PCA) - Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_PCA_Scree(X_train_Reduced, n_components=15, RANDOM_STATE=0, ax=None):\n",
    "    \"\"\"\n",
    "        :: Input(s) ::\n",
    "            X_train_Reduced - training data with only numeric columns\n",
    "            n_components - the number of principal components you would like to see\n",
    "            RANDOM_STATE - Random State of the PCA process so that repeatability is ensured\n",
    "        :: Output(s) ::\n",
    "            None\n",
    "        :: Explaination of Function ::\n",
    "        This fucntion performs Principal Components Analysis (PCA)\n",
    "        The goal here is two fold:\n",
    "            1. Generate a Scree Plot that incorporates an explained varaince ratio\n",
    "            2. Help the user determine the minumum number of components in order to consider utilizing PCA\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalize the data prior to performing PCA!\n",
    "    # This is crucial as we do not want magnitude of a feature to play a role\n",
    "    X_train_Norm = StandardScaler().fit_transform(X_train_Reduced)\n",
    "    pca = PCA(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    X_pca = pca.fit_transform(X_train_Norm)\n",
    "\n",
    "    # Since the n_comp is dynamic, we need a dynamic way to show the PC's\n",
    "    PCs = []\n",
    "    for i in range(len(X_train_Reduced.columns)):\n",
    "        PCs.append('{}'.format(i+1))\n",
    "\n",
    "    PCs = PCs[:n_components]\n",
    "\n",
    "    # Generate the Cumulative Sum of Explained Variance\n",
    "    dfCumul = pd.DataFrame({'perc_explained_var_ratio': pca.explained_variance_ratio_,\n",
    "                            'PC': PCs})\n",
    "    dfCumul['cumulative_variance'] = dfCumul['perc_explained_var_ratio'].cumsum()\n",
    "\n",
    "    # Generate the Scree Plot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "    ax.plot(dfCumul['PC'], dfCumul['perc_explained_var_ratio'], color='b', marker='o', linestyle='--', label='Variance Explained by PC')\n",
    "    ax.set_xlabel('Principal Component (PC)')\n",
    "    ax.set_ylabel('Percentage of Explained Variance')\n",
    "    ax.set_title('PCA Scree Plot')\n",
    "    ax.plot(dfCumul['PC'], dfCumul['cumulative_variance'], color='k', label=\"Cumulative Variance Explained\")\n",
    "    ax.legend(loc='upper center')\n",
    "\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 6))\n",
    "\n",
    "data_dict = {'X_train_GLOB': X_train_GLOB, 'X_train_CARB': X_train_CARB, 'X_train_SEAA': X_train_SEAA}\n",
    "\n",
    "for idx, (name, frame) in enumerate(data_dict.items()):\n",
    "    ax = axes[idx]\n",
    "    custom_PCA_Scree(frame, n_components=10, RANDOM_STATE=0, ax=ax)\n",
    "    ax.set_title(name)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis (PCA) - BiPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_BiPlot(X_train_Reduced, dimensions=2, n_comps=3, show=False, RANDOM_STATE=0):\n",
    "    \"\"\"\n",
    "        :: Input(s) ::\n",
    "            X_train_Reduced - training data with only numeric columns\n",
    "            dimensions - The number of dimensions that we can plot in (must be either 2 or 3)\n",
    "            n_comps - The number of components that you would like included in the feature extraction piece of this function\n",
    "            show - A flag which either shows the plot created or hides it\n",
    "            RANDOM_STATE - Random State of the PCA process so that repeatability is ensured\n",
    "        :: Output(s) ::\n",
    "            pca - The PCA model (giving the user to access the attributes of said model)\n",
    "            X_pca - The data points that were used to generate the figure\n",
    "            dfFeatures - A pandas dataframe containing the feature selections and feature extractions\n",
    "        :: Explaination of Function ::\n",
    "            This function creates a biplot from the PCA solution in either 2d or 3d.\n",
    "            The biplot shows the two (or three) principal components with the biggest eigenvalues (PC's that explain the most variance in the data)\n",
    "            and plots the data as well as the original features as vectors in this space.\n",
    "\n",
    "            It also returns the features from the original feature space that most closely align (via cosine similiarity) to each principal component (feature selection)\n",
    "            as well as the group of features (indicated by n_comps) that most closely aligns with each principal component which could in turn be used by the user to\n",
    "            estimate a sort of \"latent feature name\" for each PC (feature extraction)\n",
    "    \"\"\"\n",
    "    # Error Handling\n",
    "    if (dimensions < 2) or (dimensions > 3):\n",
    "        print(\"Dimensions must be either 2 or 3! Try again!\")\n",
    "        return None\n",
    "    \n",
    "    # Normalize the data prior to performing PCA!\n",
    "    # This is crucial as we do not want magnitude of a feature to play a role\n",
    "    X_train_Norm = StandardScaler().fit_transform(X_train_Reduced)\n",
    "    pca = PCA(n_components=dimensions,\n",
    "              random_state=RANDOM_STATE)\n",
    "    X_pca = pca.fit_transform(X_train_Norm)\n",
    "\n",
    "    # Generate the Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    maxdim = len(X_train_Reduced.columns)\n",
    "\n",
    "    if dimensions == 2:\n",
    "        coeff = np.transpose(pca.components_[0:2, :])\n",
    "\n",
    "        xs = X_pca[:,0]\n",
    "        ys = X_pca[:,1]\n",
    "\n",
    "        n = min(coeff.shape[0], maxdim)\n",
    "        scalex = 2.0 / (xs.max() - xs.min())\n",
    "        scaley = 2.0 / (ys.max() - ys.min())\n",
    "        text_scale_factor = 1.5\n",
    "\n",
    "        plt.scatter(xs * scalex, ys * scaley, s=5)\n",
    "\n",
    "        top_comps_PC1 = []\n",
    "        top_comps_PC2 = []\n",
    "\n",
    "        for i in range(n):\n",
    "            # TEXT PLOTTING LOGIC\n",
    "            plt.arrow(0,0,coeff[i,0],coeff[i,1],color='red',alpha=0.5)\n",
    "            plt.text(\n",
    "                coeff[i,0] * text_scale_factor,\n",
    "                coeff[i,1] * text_scale_factor,\n",
    "                X_train_Reduced.columns[i],\n",
    "                color='g',\n",
    "                ha='center',\n",
    "                va='center'\n",
    "            )\n",
    "\n",
    "            # Let's also keep track of the components that are most like each PC\n",
    "            PC1 = np.array([1,0])\n",
    "            PC2 = np.array([0,1])\n",
    "\n",
    "            featureVector = coeff[i]\n",
    "\n",
    "            cosine_similarity_PC1 = np.dot(featureVector, PC1) / (np.linalg.norm(featureVector) * np.linalg.norm(PC1))\n",
    "            cosine_similarity_PC2 = np.dot(featureVector, PC2) / (np.linalg.norm(featureVector) * np.linalg.norm(PC2))\n",
    "\n",
    "            top_comps_PC1.append((cosine_similarity_PC1, X_train_Reduced.columns[i]))\n",
    "            top_comps_PC2.append((cosine_similarity_PC2, X_train_Reduced.columns[i]))\n",
    "\n",
    "        sorted_top_comps_PC1 = sorted(top_comps_PC1, key=lambda x: x[0], reverse=True)\n",
    "        sorted_top_comps_PC2 = sorted(top_comps_PC2, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        top_comps_PC1 = [item[1] for item in sorted_top_comps_PC1]\n",
    "        top_comps_PC2 = [item[1] for item in sorted_top_comps_PC2]\n",
    "\n",
    "        outDict = {'Feature_Selection_PC1': top_comps_PC1[0],\n",
    "                   'Feature_Selection_PC2': top_comps_PC2[0],\n",
    "                   'Feature_Extraction_PC1': top_comps_PC1[:n_comps],\n",
    "                   'Feature_Extraction_PC2': top_comps_PC2[:n_comps]}\n",
    "        \n",
    "        plt.xlim(-1,1)\n",
    "        plt.ylim(-1,1)\n",
    "        plt.title('PCA Biplot')\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "        plt.grid()\n",
    "        \n",
    "        if show:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        dfFeatures = pd.DataFrame(outDict)\n",
    "\n",
    "        return pca, X_pca, dfFeatures\n",
    "    else:\n",
    "        coeff = np.transpose(pca.components_[0:3, :])\n",
    "\n",
    "        xs = X_pca[:,0]\n",
    "        ys = X_pca[:,1]\n",
    "        zs = X_pca[:,2]\n",
    "\n",
    "        n = min(coeff.shape[0], maxdim)\n",
    "        scalex = 2.0 / (xs.max() - xs.min())\n",
    "        scaley = 2.0 / (ys.max() - ys.min())\n",
    "        scalez = 2.0 / (zs.max() - zs.min())\n",
    "        text_scale_factor = 1.5\n",
    "\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.grid()\n",
    "        scatter = ax.scatter(xs * scalex,\n",
    "                             ys * scaley,\n",
    "                             zs * scalez,\n",
    "                             s=5)\n",
    "\n",
    "        top_comps_PC1 = []\n",
    "        top_comps_PC2 = []\n",
    "        top_comps_PC3 = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            # TEXT PLOTTING LOGIC\n",
    "            ax.quiver(0, 0, 0,\n",
    "                      coeff[i,0], coeff[i,1], coeff[i,2],\n",
    "                      color='red')\n",
    "            ax.text(\n",
    "                coeff[i,0] * text_scale_factor,\n",
    "                coeff[i,1] * text_scale_factor,\n",
    "                coeff[i,2] * text_scale_factor,\n",
    "                X_train_Reduced.columns[i],\n",
    "                color='g',\n",
    "                ha='center',\n",
    "                va='center'\n",
    "            )\n",
    "\n",
    "            # Let's also keep track of the components that are most like each PC\n",
    "            PC1 = np.array([1,0,0])\n",
    "            PC2 = np.array([0,1,0])\n",
    "            PC3 = np.array([0,0,1])\n",
    "\n",
    "            featureVector = coeff[i]\n",
    "\n",
    "            cosine_similarity_PC1 = np.dot(featureVector, PC1) / (np.linalg.norm(featureVector) * np.linalg.norm(PC1))\n",
    "            cosine_similarity_PC2 = np.dot(featureVector, PC2) / (np.linalg.norm(featureVector) * np.linalg.norm(PC2))\n",
    "            cosine_similarity_PC3 = np.dot(featureVector, PC3) / (np.linalg.norm(featureVector) * np.linalg.norm(PC3))\n",
    "\n",
    "            top_comps_PC1.append((cosine_similarity_PC1, X_train_Reduced.columns[i]))\n",
    "            top_comps_PC2.append((cosine_similarity_PC2, X_train_Reduced.columns[i]))\n",
    "            top_comps_PC3.append((cosine_similarity_PC3, X_train_Reduced.columns[i]))\n",
    "        \n",
    "        sorted_top_comps_PC1 = sorted(top_comps_PC1, key=lambda x: x[0], reverse=True)\n",
    "        sorted_top_comps_PC2 = sorted(top_comps_PC2, key=lambda x: x[0], reverse=True)\n",
    "        sorted_top_comps_PC3 = sorted(top_comps_PC3, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        top_comps_PC1 = [item[1] for item in sorted_top_comps_PC1]\n",
    "        top_comps_PC2 = [item[1] for item in sorted_top_comps_PC2]\n",
    "        top_comps_PC3 = [item[1] for item in sorted_top_comps_PC3]\n",
    "\n",
    "        outDict = {'Feature_Selection_PC1': top_comps_PC1[0],\n",
    "                   'Feature_Selection_PC2': top_comps_PC2[0],\n",
    "                   'Feature_Selection_PC3': top_comps_PC3[0],\n",
    "                   'Feature_Extraction_PC1': top_comps_PC1[:n_comps],\n",
    "                   'Feature_Extraction_PC2': top_comps_PC2[:n_comps],\n",
    "                   'Feature_Extraction_PC3': top_comps_PC3[:n_comps]}\n",
    "        \n",
    "        ax.set_xlim(-1,1)\n",
    "        ax.set_ylim(-1,1)\n",
    "        ax.set_zlim(-1,1)\n",
    "        ax.set_title(\"PCA Biplot\")\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_zlabel('PC3')\n",
    "        ax.set_box_aspect(aspect=None, zoom=0.9)\n",
    "\n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "        dfFeatures = pd.DataFrame(outDict)\n",
    "\n",
    "        return pca, X_pca, dfFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, X_pca, dfFeatures = custom_BiPlot(X_train_GLOB, dimensions=2, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Decision Tree - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_tree_importance(X, y, features, n_features=5):\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Create a DecisionTreeRegressor and fit it\n",
    "    model = DecisionTreeRegressor()\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # Visualize feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    plt.bar(range(n_features), importances[sorted_idx][:n_features], align=\"center\")\n",
    "    plt.xticks(range(n_features), [features[i] for i in sorted_idx[:n_features]], rotation=90)\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importance (Decision Tree)')\n",
    "    plt.show()\n",
    "\n",
    "    selected_features = [features[i] for i in sorted_idx[:n_features]]\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Call the function for both datasets\n",
    "selected_features = feature_selection_tree_importance(X_train_GLOB, y_train_GLOB, X_train_GLOB.columns,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson Correlation matrix\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "sns.heatmap(X_train_GLOB[selected_features].corr().round(2),annot=True, cmap = 'coolwarm')"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "dfcc7a431da2490fafbf0baffff270a2",
  "deepnote_persisted_session": {
   "createdAt": "2024-03-27T00:00:34.131Z"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
